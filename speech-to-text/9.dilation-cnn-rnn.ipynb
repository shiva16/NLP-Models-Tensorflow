{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_files = [f for f in os.listdir('./data') if f.endswith('.wav')]\n",
    "text_files = [f for f in os.listdir('./data') if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2800/2800 [00:24<00:00, 116.36it/s]\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = [], []\n",
    "for (wav_file, text_file) in tqdm(zip(wav_files, text_files), total = len(wav_files),ncols=80):\n",
    "    path = './data/' + wav_file\n",
    "    try:\n",
    "        y, sr = librosa.load(path, sr = None)\n",
    "    except:\n",
    "        continue\n",
    "    inputs.append(\n",
    "        librosa.feature.mfcc(\n",
    "            y = y, sr = sr, n_mfcc = 40, hop_length = int(0.05 * sr)\n",
    "        ).T\n",
    "    )\n",
    "    with open('./data/' + text_file) as f:\n",
    "        targets.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    inputs, dtype = 'float32', padding = 'post'\n",
    ")\n",
    "\n",
    "chars = list(set([c for target in targets for c in target]))\n",
    "num_classes = len(chars) + 1\n",
    "\n",
    "idx2char = {idx: char for idx, char in enumerate(chars)}\n",
    "char2idx = {char: idx for idx, char in idx2char.items()}\n",
    "\n",
    "targets = [[char2idx[c] for c in target] for target in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(inputs, epsilon=1e-8):\n",
    "    mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "    normalized = (inputs - mean) / (tf.sqrt(variance + epsilon))\n",
    "    params_shape = inputs.get_shape()[-1:]\n",
    "    gamma = tf.get_variable('gamma', params_shape, tf.float32, tf.ones_initializer())\n",
    "    beta = tf.get_variable('beta', params_shape, tf.float32, tf.zeros_initializer())\n",
    "    return gamma * normalized + beta\n",
    "\n",
    "\n",
    "def cnn_block(x, dilation_rate, pad_sz, hidden_dim, kernel_size):\n",
    "    x = layer_norm(x)\n",
    "    pad = tf.zeros([tf.shape(x)[0], pad_sz, hidden_dim])\n",
    "    x =  tf.layers.conv1d(inputs = tf.concat([pad, x, pad], 1),\n",
    "                          filters = hidden_dim,\n",
    "                          kernel_size = kernel_size,\n",
    "                          dilation_rate = dilation_rate)\n",
    "    x = x[:, :-pad_sz, :]\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    "\n",
    "class Model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        size_layers,\n",
    "        learning_rate,\n",
    "        num_features,\n",
    "        dropout = 1.0,\n",
    "        kernel_size = 5\n",
    "    ):\n",
    "        self.X = tf.placeholder(tf.float32, [None, None, num_features])\n",
    "        self.label = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y_seq_len = tf.placeholder(tf.int32, [None])\n",
    "        self.Y = tf.sparse_placeholder(tf.int32)\n",
    "        seq_lens = tf.count_nonzero(\n",
    "            tf.reduce_sum(self.X, -1), 1, dtype = tf.int32\n",
    "        )\n",
    "        batch_size = tf.shape(self.X)[0]\n",
    "        \n",
    "        x = tf.layers.conv1d(self.X, size_layers, 1)\n",
    "        for i in range(num_layers):\n",
    "            dilation_rate = 2 ** i\n",
    "            pad_sz = (kernel_size - 1) * dilation_rate\n",
    "            with tf.variable_scope('block_%d'%i):\n",
    "                x += cnn_block(x, dilation_rate, pad_sz, size_layers, kernel_size)\n",
    "        print(x)\n",
    "        \n",
    "        def cells(size, reuse = False):\n",
    "            return tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.nn.rnn_cell.GRUCell(\n",
    "                    size,\n",
    "                    reuse = reuse,\n",
    "                ),\n",
    "                state_keep_prob = dropout,\n",
    "                output_keep_prob = dropout,\n",
    "            )\n",
    "        \n",
    "        for n in range(num_layers):\n",
    "            (out_fw, out_bw), (\n",
    "                state_fw,\n",
    "                state_bw,\n",
    "            ) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cells(size_layers),\n",
    "                cell_bw = cells(size_layers),\n",
    "                inputs = x,\n",
    "                sequence_length = seq_lens,\n",
    "                dtype = tf.float32,\n",
    "                scope = 'bidirectional_rnn_%d' % (n),\n",
    "            )\n",
    "            x = tf.concat((out_fw, out_bw), 2)\n",
    "        \n",
    "        logits = tf.layers.dense(x, num_classes)\n",
    "        time_major = tf.transpose(logits, [1, 0, 2])\n",
    "        decoded, log_prob = tf.nn.ctc_beam_search_decoder(time_major, seq_lens)\n",
    "        decoded = tf.to_int32(decoded[0])\n",
    "        self.preds = tf.sparse.to_dense(decoded)\n",
    "        self.cost = tf.reduce_mean(\n",
    "            tf.nn.ctc_loss(\n",
    "                self.Y,\n",
    "                time_major,\n",
    "                seq_lens\n",
    "            )\n",
    "        )\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate = learning_rate\n",
    "        ).minimize(self.cost)\n",
    "        \n",
    "        preds = self.preds[:, :tf.reduce_max(self.Y_seq_len)]\n",
    "        masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n",
    "        preds = tf.pad(preds, [[0, 0], [0, tf.reduce_max(self.Y_seq_len)]])\n",
    "        y_t = tf.cast(preds, tf.int32)\n",
    "        self.prediction = tf.boolean_mask(y_t, masks)\n",
    "        mask_label = tf.boolean_mask(self.label, masks)\n",
    "        correct_pred = tf.equal(self.prediction, mask_label)\n",
    "        correct_index = tf.cast(correct_pred, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"block_1/add_2:0\", shape=(?, ?, 128), dtype=float32)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "size_layers = 128\n",
    "learning_rate = 1e-4\n",
    "num_layers = 2\n",
    "batch_size = 32\n",
    "epoch = 50\n",
    "\n",
    "model = Model(num_layers, size_layers, learning_rate, inputs.shape[2])\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "        seq_lens.append(len(sentence))\n",
    "    return padded_seqs, seq_lens\n",
    "\n",
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n] * len(seq), range(len(seq))))\n",
    "        values.extend(seq)\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int64)\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1] + 1], dtype=np.int64)\n",
    "\n",
    "    return indices, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 88/88 [00:38<00:00,  2.13it/s, accuracy=0.0556, cost=37.7]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:37<00:00,  2.56it/s, accuracy=0.278, cost=30.2] \n",
      "minibatch loop: 100%|██████████| 88/88 [00:39<00:00,  2.34it/s, accuracy=0.389, cost=24.2] \n",
      "minibatch loop: 100%|██████████| 88/88 [00:43<00:00,  2.22it/s, accuracy=0.722, cost=20.1] \n",
      "minibatch loop: 100%|██████████| 88/88 [00:45<00:00,  2.10it/s, accuracy=0.722, cost=17.1] \n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.09it/s, accuracy=0.722, cost=14.9] \n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.08it/s, accuracy=0.722, cost=13.8] \n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.07it/s, accuracy=0.722, cost=13.3] \n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.03it/s, accuracy=0.722, cost=13]  \n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.09it/s, accuracy=0.722, cost=12.7]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.00it/s, accuracy=0.722, cost=12.5]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.06it/s, accuracy=0.722, cost=12.3]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.02it/s, accuracy=0.722, cost=12.2]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.01it/s, accuracy=0.722, cost=12]  \n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.06it/s, accuracy=0.722, cost=11.9]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.10it/s, accuracy=0.722, cost=11.7]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.11it/s, accuracy=0.722, cost=11.5]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.11it/s, accuracy=0.722, cost=11.4]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.09it/s, accuracy=0.722, cost=11.3]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.06it/s, accuracy=0.722, cost=11.1]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.04it/s, accuracy=0.722, cost=11.1]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.02it/s, accuracy=0.722, cost=10.9]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.08it/s, accuracy=0.722, cost=10.8]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.09it/s, accuracy=0.722, cost=10.7]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.10it/s, accuracy=0.722, cost=10.6]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.02it/s, accuracy=0.722, cost=10.5]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.07it/s, accuracy=0.778, cost=10.5]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.11it/s, accuracy=0.722, cost=10.4]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.03it/s, accuracy=0.722, cost=10.3]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.02it/s, accuracy=0.722, cost=10.3]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.01it/s, accuracy=0.722, cost=10.2]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.10it/s, accuracy=0.722, cost=10.1]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.09it/s, accuracy=0.722, cost=10.1]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.09it/s, accuracy=0.722, cost=9.96]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.08it/s, accuracy=0.722, cost=9.98]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:46<00:00,  2.03it/s, accuracy=0.722, cost=9.86]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.04it/s, accuracy=0.778, cost=9.74]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.05it/s, accuracy=0.722, cost=9.65]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.03it/s, accuracy=0.833, cost=9.57]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.00it/s, accuracy=0.833, cost=9.46]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.03it/s, accuracy=0.833, cost=9.39]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.07it/s, accuracy=0.833, cost=9.34]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.09it/s, accuracy=0.833, cost=9.32]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.07it/s, accuracy=0.833, cost=9.24]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.07it/s, accuracy=0.778, cost=9.22]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.08it/s, accuracy=0.833, cost=9.13]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  1.98it/s, accuracy=0.833, cost=8.99]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.07it/s, accuracy=0.778, cost=8.88]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.01it/s, accuracy=0.833, cost=8.77]\n",
      "minibatch loop: 100%|██████████| 88/88 [00:47<00:00,  2.05it/s, accuracy=0.833, cost=8.7] \n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    pbar = tqdm(\n",
    "        range(0, len(inputs), batch_size), desc = 'minibatch loop')\n",
    "    for i in pbar:\n",
    "        batch_x = inputs[i : min(i + batch_size, len(inputs))]\n",
    "        y = targets[i : min(i + batch_size, len(inputs))]\n",
    "        batch_y = sparse_tuple_from(y)\n",
    "        batch_label, batch_len = pad_sentence_batch(y, 0)\n",
    "        _, cost, accuracy = sess.run(\n",
    "            [model.optimizer, model.cost, model.accuracy],\n",
    "            feed_dict = {model.X: batch_x, model.Y: batch_y, \n",
    "                         model.label: batch_label, model.Y_seq_len: batch_len},\n",
    "        )\n",
    "\n",
    "        accuracy = sess.run(model.accuracy, feed_dict = {model.X: batch_x[: 1],\n",
    "                                                            model.label: batch_label[: 1],\n",
    "                                                            model.Y_seq_len: batch_len[: 1]})\n",
    "        \n",
    "        \n",
    "        pbar.set_postfix(cost = cost, accuracy = np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real: say the word wire\n",
      "predicted: say the word ae\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_index = random.randint(0, len(targets) - 1)\n",
    "batch_x = inputs[random_index : random_index + 1]\n",
    "print(\n",
    "    'real:',\n",
    "    ''.join(\n",
    "        [idx2char[no] for no in targets[random_index : random_index + 1][0]]\n",
    "    ),\n",
    ")\n",
    "batch_y = sparse_tuple_from(targets[random_index : random_index + 1])\n",
    "pred = sess.run(model.preds, feed_dict = {model.X: batch_x})[0]\n",
    "print('predicted:', ''.join([idx2char[no] for no in pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
